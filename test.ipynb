{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\orpheus\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Model: unsloth/orpheus-3b-0.1-pretrained\n",
      "Loading Tokenizer\n",
      "Loaded Tokenizer from File\n",
      "Setting device to cuda\n",
      "Device now set to cuda\n",
      "Loading snac\n",
      "Completed loading snac\n",
      "Loading model: unsloth/orpheus-3b-0.1-pretrained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Loading Model\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments, AutoTokenizer\n",
    "import torch\n",
    "from snac import SNAC\n",
    "import librosa\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "model_name = \"unsloth/orpheus-3b-0.1-pretrained\"\n",
    "model_path = \"models/orpheus\"\n",
    "print(f\"Using Model: {model_name}\")\n",
    "\n",
    "print(\"Loading Tokenizer\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "print(\"Loaded Tokenizer from File\")\n",
    "\n",
    "print(\"Setting device to cuda\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device now set to {device}\")\n",
    "\n",
    "print(\"Loading snac\")\n",
    "snac_model = SNAC.from_pretrained(\"hubertsiuzdak/snac_24khz\")\n",
    "print(\"Completed loading snac\")\n",
    "\n",
    "print(f\"Loading model: {model_name}\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16)\n",
    "model.eval()\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to('cuda')\n",
    "print(\"Completed Loading Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_audio(audio_file_path, snac_model):\n",
    "    audio_array, sample_rate = librosa.load(audio_file_path, sr=24000)\n",
    "    waveform = torch.from_numpy(audio_array).unsqueeze(0)\n",
    "    waveform = waveform.to(dtype=torch.float32)\n",
    "\n",
    "    waveform = waveform.unsqueeze(0)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        codes = snac_model.encode(waveform)\n",
    "\n",
    "    all_codes = []\n",
    "    for i in range(codes[0].shape[1]):\n",
    "        all_codes.append(codes[0][0][i].item() + 128266)\n",
    "        all_codes.append(codes[1][0][2 * i].item() + 128266 + 4096)\n",
    "        all_codes.append(codes[2][0][4 * i].item() + 128266 + (2 * 4096))\n",
    "        all_codes.append(codes[2][0][(4 * i) + 1].item() + 128266 + (3 * 4096))\n",
    "        all_codes.append(codes[1][0][(2 * i) + 1].item() + 128266 + (4 * 4096))\n",
    "        all_codes.append(codes[2][0][(4 * i) + 2].item() + 128266 + (5 * 4096))\n",
    "        all_codes.append(codes[2][0][(4 * i) + 3].item() + 128266 + (6 * 4096))\n",
    "\n",
    "    return all_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_inputs(\n",
    "    fpath_audio_ref,\n",
    "    audio_ref_transcript: str,\n",
    "    text_prompts: list[str],\n",
    "    snac_model,\n",
    "    tokenizer,\n",
    "):\n",
    "    audio_tokens = tokenize_audio(fpath_audio_ref, snac_model)\n",
    "\n",
    "    start_tokens = torch.tensor([[128259]], dtype=torch.int64)\n",
    "    end_tokens = torch.tensor([[128009, 128260, 128261, 128257]], dtype=torch.int64)\n",
    "    final_tokens = torch.tensor([[128258, 128262]], dtype=torch.int64)\n",
    "\n",
    "    transcript_tokens = tokenizer(audio_ref_transcript, return_tensors=\"pt\")\n",
    "\n",
    "    # REF PROMPT TOKENS could be precomputed\n",
    "    input_ids = transcript_tokens['input_ids']\n",
    "    zeroprompt_input_ids = torch.cat([start_tokens, input_ids, end_tokens, torch.tensor([audio_tokens]), final_tokens],\n",
    "                                     dim=1)  # SOH SOT Text EOT EOH\n",
    "\n",
    "    # PROMPT TOKENS (what to say)\n",
    "    all_modified_input_ids = []\n",
    "    for prompt in text_prompts:\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "        second_input_ids = torch.cat([zeroprompt_input_ids, start_tokens, input_ids, end_tokens], dim=1)\n",
    "        all_modified_input_ids.append(second_input_ids)\n",
    "\n",
    "    all_padded_tensors = []\n",
    "    all_attention_masks = []\n",
    "    max_length = max([modified_input_ids.shape[1] for modified_input_ids in all_modified_input_ids])\n",
    "\n",
    "    for modified_input_ids in all_modified_input_ids:\n",
    "        padding = max_length - modified_input_ids.shape[1]\n",
    "        padded_tensor = torch.cat([torch.full((1, padding), 128263, dtype=torch.int64), modified_input_ids], dim=1)\n",
    "        attention_mask = torch.cat([torch.zeros((1, padding), dtype=torch.int64),\n",
    "                                    torch.ones((1, modified_input_ids.shape[1]), dtype=torch.int64)], dim=1)\n",
    "        all_padded_tensors.append(padded_tensor)\n",
    "        all_attention_masks.append(attention_mask)\n",
    "\n",
    "    all_padded_tensors = torch.cat(all_padded_tensors, dim=0)\n",
    "    all_attention_masks = torch.cat(all_attention_masks, dim=0)\n",
    "\n",
    "    input_ids = all_padded_tensors.to(\"cuda\")\n",
    "    attention_mask = all_attention_masks.to(\"cuda\")\n",
    "    return input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please pass your input's `attention_mask` to obtain reliable results.\n",
    "# Setting `pad_token_id` to `eos_token_id`:128258 for open-end generation.\n",
    "def inference(model, input_ids, attention_mask):\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=990,\n",
    "            do_sample=True,\n",
    "            temperature=0.5,\n",
    "            # top_k=40,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=128258,\n",
    "            # end_token_id=128009\n",
    "        )\n",
    "\n",
    "        # dunno\n",
    "        # generated_ids = torch.cat([generated_ids, torch.tensor([[128262]]).to(\"cuda\")], dim=1) # EOAI\n",
    "\n",
    "        return generated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redistribute_codes(code_list, snac_model, debug=True):\n",
    "    \"\"\"\n",
    "    Redistribute codes for SNAC model and generate audio.\n",
    "    \n",
    "    Parameters:\n",
    "    code_list (list): List of codes to redistribute\n",
    "    snac_model: SNAC model for decoding\n",
    "    debug (bool): Whether to print debug information\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: Generated audio waveform\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    \n",
    "    # Make a copy to avoid modifying the original\n",
    "    code_list = code_list.copy()\n",
    "    \n",
    "    # Filter out invalid codes (negative values) and replace with zeros\n",
    "    for i in range(len(code_list)):\n",
    "        if code_list[i] < 0:\n",
    "            code_list[i] = 0\n",
    "    \n",
    "    # SNAC expects codes in multiples of 7\n",
    "    # If not divisible by 7, add padding\n",
    "    if len(code_list) % 7 != 0:\n",
    "        padding_needed = 7 - (len(code_list) % 7)\n",
    "        code_list.extend([0] * padding_needed)\n",
    "    \n",
    "    # Verify we have enough codes to work with\n",
    "    if len(code_list) < 7:\n",
    "        if debug:\n",
    "            print(\"Warning: Not enough codes. Padding with zeros.\")\n",
    "        code_list = code_list + [0] * (7 - len(code_list))\n",
    "    \n",
    "    # Define vocab size for each layer\n",
    "    vocab_size = 4096  # Standard SNAC vocabulary size per layer\n",
    "    \n",
    "    # Initialize containers for each layer\n",
    "    layer_1 = []\n",
    "    layer_2 = []\n",
    "    layer_3 = []\n",
    "    \n",
    "    # Carefully process each group of 7 codes\n",
    "    for i in range(len(code_list) // 7):\n",
    "        start_idx = 7 * i\n",
    "        \n",
    "        # Safety check for each access to avoid index errors\n",
    "        if start_idx < len(code_list):\n",
    "            # First code goes to layer 1\n",
    "            idx_0 = min(max(0, code_list[start_idx]), vocab_size - 1)\n",
    "            layer_1.append(idx_0)\n",
    "            \n",
    "            # Second code goes to layer 2\n",
    "            if start_idx + 1 < len(code_list):\n",
    "                idx_1 = min(max(0, code_list[start_idx + 1] - vocab_size), vocab_size - 1)\n",
    "                layer_2.append(idx_1)\n",
    "            else:\n",
    "                layer_2.append(0)  # Pad with zero if index out of range\n",
    "            \n",
    "            # Third code goes to layer 3\n",
    "            if start_idx + 2 < len(code_list):\n",
    "                idx_2 = min(max(0, code_list[start_idx + 2] - (2 * vocab_size)), vocab_size - 1)\n",
    "                layer_3.append(idx_2)\n",
    "            else:\n",
    "                layer_3.append(0)\n",
    "            \n",
    "            # Fourth code goes to layer 3\n",
    "            if start_idx + 3 < len(code_list):\n",
    "                idx_3 = min(max(0, code_list[start_idx + 3] - (3 * vocab_size)), vocab_size - 1)\n",
    "                layer_3.append(idx_3)\n",
    "            else:\n",
    "                layer_3.append(0)\n",
    "            \n",
    "            # Fifth code goes to layer 2\n",
    "            if start_idx + 4 < len(code_list):\n",
    "                idx_4 = min(max(0, code_list[start_idx + 4] - (4 * vocab_size)), vocab_size - 1)\n",
    "                layer_2.append(idx_4)\n",
    "            else:\n",
    "                layer_2.append(0)\n",
    "            \n",
    "            # Sixth code goes to layer 3\n",
    "            if start_idx + 5 < len(code_list):\n",
    "                idx_5 = min(max(0, code_list[start_idx + 5] - (5 * vocab_size)), vocab_size - 1)\n",
    "                layer_3.append(idx_5)\n",
    "            else:\n",
    "                layer_3.append(0)\n",
    "            \n",
    "            # Seventh code goes to layer 3\n",
    "            if start_idx + 6 < len(code_list):\n",
    "                idx_6 = min(max(0, code_list[start_idx + 6] - (6 * vocab_size)), vocab_size - 1)\n",
    "                layer_3.append(idx_6)\n",
    "            else:\n",
    "                layer_3.append(0)\n",
    "    \n",
    "    # Debug info\n",
    "    if debug:\n",
    "        print(f\"Layer 1 length: {len(layer_1)}, range: {min(layer_1) if layer_1 else 0}-{max(layer_1) if layer_1 else 0}\")\n",
    "        print(f\"Layer 2 length: {len(layer_2)}, range: {min(layer_2) if layer_2 else 0}-{max(layer_2) if layer_2 else 0}\")\n",
    "        print(f\"Layer 3 length: {len(layer_3)}, range: {min(layer_3) if layer_3 else 0}-{max(layer_3) if layer_3 else 0}\")\n",
    "    \n",
    "    # Create tensors with proper shapes and correct dtype\n",
    "    codes = [\n",
    "        torch.tensor(layer_1, dtype=torch.long).unsqueeze(0),\n",
    "        torch.tensor(layer_2, dtype=torch.long).unsqueeze(0),\n",
    "        torch.tensor(layer_3, dtype=torch.long).unsqueeze(0)\n",
    "    ]\n",
    "    \n",
    "    # Attempt to decode with the model\n",
    "    try:\n",
    "        audio_hat = snac_model.decode(codes)\n",
    "        if debug:\n",
    "            print(f\"Successfully generated audio with shape: {audio_hat.shape}\")\n",
    "        return audio_hat\n",
    "    except Exception as e:\n",
    "        print(f\"Error during decoding: {e}\")\n",
    "        \n",
    "        # Progressive fallback strategy with multiple attempts\n",
    "        try:\n",
    "            # First fallback: Try with minimal valid inputs\n",
    "            fallback_codes = [\n",
    "                torch.tensor([0], dtype=torch.long).unsqueeze(0),  # Layer 1\n",
    "                torch.tensor([0, 0], dtype=torch.long).unsqueeze(0),  # Layer 2\n",
    "                torch.tensor([0, 0, 0, 0], dtype=torch.long).unsqueeze(0)  # Layer 3\n",
    "            ]\n",
    "            return snac_model.decode(fallback_codes)\n",
    "        except Exception as e2:\n",
    "            print(f\"First fallback failed: {e2}\")\n",
    "            \n",
    "            # Second fallback: Try with model-specific defaults if available\n",
    "            try:\n",
    "                # Some SNAC models have default values that can be used\n",
    "                silence = getattr(snac_model, 'get_silence', lambda: None)()\n",
    "                if silence is not None:\n",
    "                    return silence\n",
    "                else:\n",
    "                    # Last resort: Generate simple sine wave as placeholder\n",
    "                    import torch\n",
    "                    import math\n",
    "                    sample_rate = 24000  # Common for SNAC models\n",
    "                    duration = 1  # 1 second\n",
    "                    t = torch.arange(0, duration, 1/sample_rate)\n",
    "                    sine_wave = torch.sin(2 * math.pi * 440 * t)  # 440 Hz sine wave\n",
    "                    return sine_wave.unsqueeze(0)\n",
    "            except Exception as e3:\n",
    "                print(f\"All fallbacks failed: {e3}\")\n",
    "                return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tokens_to_speech(generated_ids, snac_model):\n",
    "    token_to_find = 128257\n",
    "    token_to_remove = 128258\n",
    "    token_indices = (generated_ids == token_to_find).nonzero(as_tuple=True)\n",
    "\n",
    "    if len(token_indices[1]) > 0:\n",
    "        last_occurrence_idx = token_indices[1][-1].item()\n",
    "        cropped_tensor = generated_ids[:, last_occurrence_idx + 1:]\n",
    "    else:\n",
    "        cropped_tensor = generated_ids\n",
    "\n",
    "    _mask = cropped_tensor != token_to_remove\n",
    "    processed_rows = []\n",
    "    for row in cropped_tensor:\n",
    "        # Apply the mask to each row\n",
    "        masked_row = row[row != token_to_remove]\n",
    "        processed_rows.append(masked_row)\n",
    "\n",
    "    code_lists = []\n",
    "    for row in processed_rows:\n",
    "        # row is a 1D tensor with its own length\n",
    "        row_length = row.size(0)\n",
    "        new_length = (row_length // 7) * 7  # largest multiple of 7 that fits in this row\n",
    "        trimmed_row = row[:new_length]\n",
    "        trimmed_row = [t - 128266 for t in trimmed_row]\n",
    "        code_lists.append(trimmed_row)\n",
    "\n",
    "    my_samples = []\n",
    "    for code_list in code_lists:\n",
    "        samples = redistribute_codes(code_list, snac_model)\n",
    "        my_samples.append(samples)\n",
    "\n",
    "    return my_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_wav_from(samples: list) -> list[np.ndarray]:\n",
    "    \"\"\"Converts a list of PyTorch tensors (or NumPy arrays) to NumPy arrays.\"\"\"\n",
    "    processed_samples = []\n",
    "\n",
    "    for s in samples:\n",
    "        if isinstance(s, torch.Tensor):  # Check if it's a tensor\n",
    "            s = s.detach().squeeze().to('cpu').numpy()\n",
    "        else:  # Assume it's already a NumPy array\n",
    "            s = np.squeeze(s)\n",
    "\n",
    "        processed_samples.append(s)\n",
    "\n",
    "    return processed_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_tts(fpath_audio_ref, audio_ref_transcript, texts: list[str], model, snac_model, tokenizer):\n",
    "    inp_ids, attn_mask = prepare_inputs(fpath_audio_ref, audio_ref_transcript, texts, snac_model, tokenizer)\n",
    "    gen_ids = inference(model, inp_ids, attn_mask)\n",
    "    samples = convert_tokens_to_speech(gen_ids, snac_model)\n",
    "    wav_forms = to_wav_from(samples)\n",
    "    return wav_forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_wav(samples: list[np.array], sample_rate: int, filenames: list[str]):\n",
    "    \"\"\" Saves a list of tensors as .wav files.\n",
    "\n",
    "    Args:\n",
    "        samples (list[torch.Tensor]): List of audio tensors.\n",
    "        sample_rate (int): Sample rate in Hz.\n",
    "        filenames (list[str]): List of filenames to save.\n",
    "    \"\"\"\n",
    "    wav_data = to_wav_from(samples)\n",
    "\n",
    "    for data, filename in zip(wav_data, filenames):\n",
    "        write(filename, sample_rate, data.astype(np.float32))\n",
    "        print(f\"saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero shot: pre_audio/02.wav This is another phrase that will come in very handy for you all when it comes to sounding polite.\n",
      "Layer 1 length: 81, range: 0-4087\n",
      "Layer 2 length: 162, range: 0-4085\n",
      "Layer 3 length: 324, range: 0-4082\n",
      "Successfully generated audio with shape: torch.Size([1, 1, 165888])\n",
      "Layer 1 length: 81, range: 48-4091\n",
      "Layer 2 length: 162, range: 19-4080\n",
      "Layer 3 length: 324, range: 14-4079\n",
      "Successfully generated audio with shape: torch.Size([1, 1, 165888])\n",
      "Layer 1 length: 81, range: 0-4095\n",
      "Layer 2 length: 162, range: 0-4060\n",
      "Layer 3 length: 324, range: 0-4065\n",
      "Successfully generated audio with shape: torch.Size([1, 1, 165888])\n",
      "saved to out/02_0.wav\n",
      "saved to out/02_1.wav\n",
      "saved to out/02_2.wav\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"He questioned what'll be the result of the experiment.\",\n",
    "    \"He would'nt have done it if he knew the consequences.\",\n",
    "    \"If we were'nt focused, wed miss the opportunity.\",\n",
    "    \"She would'nt even try to resolve the issue.\",\n",
    "    # \"They were'nt always this careful with their work.\",\n",
    "    # \"They were'nt participating in the survey.\",\n",
    "    # \"They've attended the conference.\",\n",
    "    # \"They've made their decision.\",\n",
    "    # \"We were'nt able to find the solution.\",\n",
    "    # \"We'd like to go hiking this weekend.\",\n",
    "    # \"We'd worked all day on the project.\",\n",
    "    # \"We're concerned about the upcoming deadline.\",\n",
    "]\n",
    "\n",
    "prompt_pairs = [\n",
    "    # (\"pre_audio/01.wav\", \"This British etiquette is often passed down through many generations from family to family to family, which has led to a very strong cultural emphasis on politeness. Politeness even in everyday interactions and being mindful about not causing inconvenience to your fellow human beings.\"),\n",
    "    (\"pre_audio/02.wav\", \"This is another phrase that will come in very handy for you all when it comes to sounding polite.\"),\n",
    "    # (\"pre_audio/03.wav\", \"A classic example is when you're at the dinner table and you ask someone to pass you the salt.\")\n",
    "]\n",
    "\n",
    "for fpath_audio, audio_transcript in prompt_pairs:\n",
    "    print(f\"zero shot: {fpath_audio} {audio_transcript}\")\n",
    "    wav_forms = zero_shot_tts(fpath_audio, audio_transcript, texts, model, snac_model, tokenizer)\n",
    "\n",
    "    # import os\n",
    "    from pathlib import Path\n",
    "\n",
    "\n",
    "    out_dir = Path('out')\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)  # Correct method\n",
    "    file_names = [f\"{out_dir.as_posix()}/{Path(fpath_audio).stem}_{i}.wav\" for i, t in enumerate(texts)]\n",
    "    save_wav(wav_forms, 24000, file_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
