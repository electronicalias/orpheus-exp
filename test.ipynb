{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Model: unsloth/orpheus-3b-0.1-pretrained\n",
      "Loading Tokenizer\n",
      "Loaded Tokenizer from File\n",
      "Setting device to cuda\n",
      "Device now set to cuda\n",
      "Loading snac\n",
      "Completed loading snac\n",
      "Loading model: unsloth/orpheus-3b-0.1-pretrained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Loading Model\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments, AutoTokenizer\n",
    "import torch\n",
    "from snac import SNAC\n",
    "import librosa\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "model_name = \"unsloth/orpheus-3b-0.1-pretrained\"\n",
    "model_path = \"models/orpheus\"\n",
    "print(f\"Using Model: {model_name}\")\n",
    "\n",
    "print(\"Loading Tokenizer\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "print(\"Loaded Tokenizer from File\")\n",
    "\n",
    "print(\"Setting device to cuda\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device now set to {device}\")\n",
    "\n",
    "print(\"Loading snac\")\n",
    "snac_model = SNAC.from_pretrained(\"hubertsiuzdak/snac_24khz\")\n",
    "print(\"Completed loading snac\")\n",
    "\n",
    "print(f\"Loading model: {model_name}\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16)\n",
    "model.eval()\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to('cuda')\n",
    "print(\"Completed Loading Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_audio(audio_file_path, snac_model):\n",
    "    audio_array, sample_rate = librosa.load(audio_file_path, sr=24000)\n",
    "    waveform = torch.from_numpy(audio_array).unsqueeze(0)\n",
    "    waveform = waveform.to(dtype=torch.float32)\n",
    "\n",
    "    waveform = waveform.unsqueeze(0)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        codes = snac_model.encode(waveform)\n",
    "\n",
    "    all_codes = []\n",
    "    for i in range(codes[0].shape[1]):\n",
    "        all_codes.append(codes[0][0][i].item() + 128266)\n",
    "        all_codes.append(codes[1][0][2 * i].item() + 128266 + 4096)\n",
    "        all_codes.append(codes[2][0][4 * i].item() + 128266 + (2 * 4096))\n",
    "        all_codes.append(codes[2][0][(4 * i) + 1].item() + 128266 + (3 * 4096))\n",
    "        all_codes.append(codes[1][0][(2 * i) + 1].item() + 128266 + (4 * 4096))\n",
    "        all_codes.append(codes[2][0][(4 * i) + 2].item() + 128266 + (5 * 4096))\n",
    "        all_codes.append(codes[2][0][(4 * i) + 3].item() + 128266 + (6 * 4096))\n",
    "\n",
    "    return all_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_inputs(\n",
    "    fpath_audio_ref,\n",
    "    audio_ref_transcript: str,\n",
    "    text_prompts: list[str],\n",
    "    snac_model,\n",
    "    tokenizer,\n",
    "):\n",
    "    audio_tokens = tokenize_audio(fpath_audio_ref, snac_model)\n",
    "\n",
    "    start_tokens = torch.tensor([[128259]], dtype=torch.int64)\n",
    "    end_tokens = torch.tensor([[128009, 128260, 128261, 128257]], dtype=torch.int64)\n",
    "    final_tokens = torch.tensor([[128258, 128262]], dtype=torch.int64)\n",
    "\n",
    "    transcript_tokens = tokenizer(audio_ref_transcript, return_tensors=\"pt\")\n",
    "\n",
    "    # REF PROMPT TOKENS could be precomputed\n",
    "    input_ids = transcript_tokens['input_ids']\n",
    "    zeroprompt_input_ids = torch.cat([start_tokens, input_ids, end_tokens, torch.tensor([audio_tokens]), final_tokens],\n",
    "                                     dim=1)  # SOH SOT Text EOT EOH\n",
    "\n",
    "    # PROMPT TOKENS (what to say)\n",
    "    all_modified_input_ids = []\n",
    "    for prompt in text_prompts:\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "        second_input_ids = torch.cat([zeroprompt_input_ids, start_tokens, input_ids, end_tokens], dim=1)\n",
    "        all_modified_input_ids.append(second_input_ids)\n",
    "\n",
    "    all_padded_tensors = []\n",
    "    all_attention_masks = []\n",
    "    max_length = max([modified_input_ids.shape[1] for modified_input_ids in all_modified_input_ids])\n",
    "\n",
    "    for modified_input_ids in all_modified_input_ids:\n",
    "        padding = max_length - modified_input_ids.shape[1]\n",
    "        padded_tensor = torch.cat([torch.full((1, padding), 128263, dtype=torch.int64), modified_input_ids], dim=1)\n",
    "        attention_mask = torch.cat([torch.zeros((1, padding), dtype=torch.int64),\n",
    "                                    torch.ones((1, modified_input_ids.shape[1]), dtype=torch.int64)], dim=1)\n",
    "        all_padded_tensors.append(padded_tensor)\n",
    "        all_attention_masks.append(attention_mask)\n",
    "\n",
    "    all_padded_tensors = torch.cat(all_padded_tensors, dim=0)\n",
    "    all_attention_masks = torch.cat(all_attention_masks, dim=0)\n",
    "\n",
    "    input_ids = all_padded_tensors.to(\"cuda\")\n",
    "    attention_mask = all_attention_masks.to(\"cuda\")\n",
    "    return input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please pass your input's `attention_mask` to obtain reliable results.\n",
    "# Setting `pad_token_id` to `eos_token_id`:128258 for open-end generation.\n",
    "def inference(model, input_ids, attention_mask):\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=450,\n",
    "            do_sample=True,\n",
    "            temperature=0.5,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=128258,\n",
    "        )\n",
    "\n",
    "        # dunno\n",
    "        # generated_ids = torch.cat([generated_ids, torch.tensor([[128262]]).to(\"cuda\")], dim=1) # EOAI\n",
    "\n",
    "        return generated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redistribute_codes(code_list, snac_model, debug=True):\n",
    "    \"\"\"\n",
    "    Redistribute codes for SNAC model and generate audio.\n",
    "    \n",
    "    Parameters:\n",
    "    code_list (list): List of codes to redistribute\n",
    "    snac_model: SNAC model for decoding\n",
    "    debug (bool): Whether to print debug information\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: Generated audio waveform\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    \n",
    "    # Make a copy to avoid modifying the original\n",
    "    code_list = code_list.copy()\n",
    "    \n",
    "    # Filter out invalid codes (negative values) and replace with zeros\n",
    "    for i in range(len(code_list)):\n",
    "        if code_list[i] < 0:\n",
    "            code_list[i] = 0\n",
    "    \n",
    "    # SNAC expects codes in multiples of 7\n",
    "    # If not divisible by 7, add padding\n",
    "    if len(code_list) % 7 != 0:\n",
    "        padding_needed = 7 - (len(code_list) % 7)\n",
    "        code_list.extend([0] * padding_needed)\n",
    "    \n",
    "    # Verify we have enough codes to work with\n",
    "    if len(code_list) < 7:\n",
    "        if debug:\n",
    "            print(\"Warning: Not enough codes. Padding with zeros.\")\n",
    "        code_list = code_list + [0] * (7 - len(code_list))\n",
    "    \n",
    "    # Define vocab size for each layer\n",
    "    vocab_size = 4096  # Standard SNAC vocabulary size per layer\n",
    "    \n",
    "    # Initialize containers for each layer\n",
    "    layer_1 = []\n",
    "    layer_2 = []\n",
    "    layer_3 = []\n",
    "    \n",
    "    # Carefully process each group of 7 codes\n",
    "    for i in range(len(code_list) // 7):\n",
    "        start_idx = 7 * i\n",
    "        \n",
    "        # Safety check for each access to avoid index errors\n",
    "        if start_idx < len(code_list):\n",
    "            # First code goes to layer 1\n",
    "            idx_0 = min(max(0, code_list[start_idx]), vocab_size - 1)\n",
    "            layer_1.append(idx_0)\n",
    "            \n",
    "            # Second code goes to layer 2\n",
    "            if start_idx + 1 < len(code_list):\n",
    "                idx_1 = min(max(0, code_list[start_idx + 1] - vocab_size), vocab_size - 1)\n",
    "                layer_2.append(idx_1)\n",
    "            else:\n",
    "                layer_2.append(0)  # Pad with zero if index out of range\n",
    "            \n",
    "            # Third code goes to layer 3\n",
    "            if start_idx + 2 < len(code_list):\n",
    "                idx_2 = min(max(0, code_list[start_idx + 2] - (2 * vocab_size)), vocab_size - 1)\n",
    "                layer_3.append(idx_2)\n",
    "            else:\n",
    "                layer_3.append(0)\n",
    "            \n",
    "            # Fourth code goes to layer 3\n",
    "            if start_idx + 3 < len(code_list):\n",
    "                idx_3 = min(max(0, code_list[start_idx + 3] - (3 * vocab_size)), vocab_size - 1)\n",
    "                layer_3.append(idx_3)\n",
    "            else:\n",
    "                layer_3.append(0)\n",
    "            \n",
    "            # Fifth code goes to layer 2\n",
    "            if start_idx + 4 < len(code_list):\n",
    "                idx_4 = min(max(0, code_list[start_idx + 4] - (4 * vocab_size)), vocab_size - 1)\n",
    "                layer_2.append(idx_4)\n",
    "            else:\n",
    "                layer_2.append(0)\n",
    "            \n",
    "            # Sixth code goes to layer 3\n",
    "            if start_idx + 5 < len(code_list):\n",
    "                idx_5 = min(max(0, code_list[start_idx + 5] - (5 * vocab_size)), vocab_size - 1)\n",
    "                layer_3.append(idx_5)\n",
    "            else:\n",
    "                layer_3.append(0)\n",
    "            \n",
    "            # Seventh code goes to layer 3\n",
    "            if start_idx + 6 < len(code_list):\n",
    "                idx_6 = min(max(0, code_list[start_idx + 6] - (6 * vocab_size)), vocab_size - 1)\n",
    "                layer_3.append(idx_6)\n",
    "            else:\n",
    "                layer_3.append(0)\n",
    "    \n",
    "    # Debug info\n",
    "    if debug:\n",
    "        print(f\"Layer 1 length: {len(layer_1)}, range: {min(layer_1) if layer_1 else 0}-{max(layer_1) if layer_1 else 0}\")\n",
    "        print(f\"Layer 2 length: {len(layer_2)}, range: {min(layer_2) if layer_2 else 0}-{max(layer_2) if layer_2 else 0}\")\n",
    "        print(f\"Layer 3 length: {len(layer_3)}, range: {min(layer_3) if layer_3 else 0}-{max(layer_3) if layer_3 else 0}\")\n",
    "    \n",
    "    # Create tensors with proper shapes and correct dtype\n",
    "    codes = [\n",
    "        torch.tensor(layer_1, dtype=torch.long).unsqueeze(0),\n",
    "        torch.tensor(layer_2, dtype=torch.long).unsqueeze(0),\n",
    "        torch.tensor(layer_3, dtype=torch.long).unsqueeze(0)\n",
    "    ]\n",
    "    \n",
    "    # Attempt to decode with the model\n",
    "    try:\n",
    "        audio_hat = snac_model.decode(codes)\n",
    "        if debug:\n",
    "            print(f\"Successfully generated audio with shape: {audio_hat.shape}\")\n",
    "        return audio_hat\n",
    "    except Exception as e:\n",
    "        print(f\"Error during decoding: {e}\")\n",
    "        \n",
    "        # Progressive fallback strategy with multiple attempts\n",
    "        try:\n",
    "            # First fallback: Try with minimal valid inputs\n",
    "            fallback_codes = [\n",
    "                torch.tensor([0], dtype=torch.long).unsqueeze(0),  # Layer 1\n",
    "                torch.tensor([0, 0], dtype=torch.long).unsqueeze(0),  # Layer 2\n",
    "                torch.tensor([0, 0, 0, 0], dtype=torch.long).unsqueeze(0)  # Layer 3\n",
    "            ]\n",
    "            return snac_model.decode(fallback_codes)\n",
    "        except Exception as e2:\n",
    "            print(f\"First fallback failed: {e2}\")\n",
    "            \n",
    "            # Second fallback: Try with model-specific defaults if available\n",
    "            try:\n",
    "                # Some SNAC models have default values that can be used\n",
    "                silence = getattr(snac_model, 'get_silence', lambda: None)()\n",
    "                if silence is not None:\n",
    "                    return silence\n",
    "                else:\n",
    "                    # Last resort: Generate simple sine wave as placeholder\n",
    "                    import torch\n",
    "                    import math\n",
    "                    sample_rate = 24000  # Common for SNAC models\n",
    "                    duration = 1  # 1 second\n",
    "                    t = torch.arange(0, duration, 1/sample_rate)\n",
    "                    sine_wave = torch.sin(2 * math.pi * 440 * t)  # 440 Hz sine wave\n",
    "                    return sine_wave.unsqueeze(0)\n",
    "            except Exception as e3:\n",
    "                print(f\"All fallbacks failed: {e3}\")\n",
    "                return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_remove_buzzing(audio_tensor, threshold=0.01, min_frames=1000):\n",
    "    \"\"\"\n",
    "    Detects and removes buzzing artifacts at the end of audio\n",
    "    \n",
    "    Parameters:\n",
    "    audio_tensor: Audio tensor\n",
    "    threshold: Energy threshold for detecting buzzing\n",
    "    min_frames: Minimum number of frames to keep\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: Cleaned audio tensor\n",
    "    \"\"\"\n",
    "    # Ensure we have at least 2D tensor\n",
    "    if audio_tensor.dim() < 2:\n",
    "        audio_tensor = audio_tensor.unsqueeze(0)\n",
    "    \n",
    "    # Get the audio data\n",
    "    audio = audio_tensor.squeeze()\n",
    "    \n",
    "    # Calculate frame energy\n",
    "    frame_size = 512\n",
    "    hop_size = 256\n",
    "    num_frames = (audio.shape[0] - frame_size) // hop_size + 1\n",
    "    \n",
    "    # If audio is too short, return as is\n",
    "    if num_frames < 10:\n",
    "        return audio_tensor\n",
    "    \n",
    "    energies = []\n",
    "    for i in range(num_frames):\n",
    "        start = i * hop_size\n",
    "        end = start + frame_size\n",
    "        frame = audio[start:end]\n",
    "        energy = torch.mean(frame ** 2)\n",
    "        energies.append(energy.item())\n",
    "    \n",
    "    # Find where the energy drops significantly\n",
    "    # Start from the end and move backwards\n",
    "    end_idx = len(energies) - 1\n",
    "    for i in range(len(energies) - 2, 0, -1):\n",
    "        if energies[i] > threshold and energies[i+1] < threshold:\n",
    "            end_idx = i\n",
    "            break\n",
    "    \n",
    "    # Convert frame index to sample index\n",
    "    end_sample = min(end_idx * hop_size + frame_size, audio.shape[0])\n",
    "    \n",
    "    # Ensure we keep at least min_frames\n",
    "    end_sample = max(end_sample, min_frames)\n",
    "    \n",
    "    # Apply a short fade out\n",
    "    fade_len = min(1000, end_sample // 10)\n",
    "    if fade_len > 0:\n",
    "        fade_out = torch.linspace(1, 0, fade_len)\n",
    "        fade_start = end_sample - fade_len\n",
    "        \n",
    "        if audio.dim() == 1:\n",
    "            # For 1D tensor\n",
    "            audio[fade_start:end_sample] *= fade_out\n",
    "            audio[end_sample:] = 0\n",
    "        else:\n",
    "            # For 2D tensor (batch, samples)\n",
    "            for i in range(audio.shape[0]):\n",
    "                audio[i, fade_start:end_sample] *= fade_out\n",
    "                audio[i, end_sample:] = 0\n",
    "    \n",
    "    return audio_tensor[:, :end_sample] if audio_tensor.dim() > 1 else audio_tensor[:end_sample]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tokens_to_speech(generated_ids, snac_model):\n",
    "    token_to_find = 128257  # EOH token\n",
    "    token_to_remove = 128258  # EOS token\n",
    "    \n",
    "    # Find EOH token position\n",
    "    token_indices = (generated_ids == token_to_find).nonzero(as_tuple=True)\n",
    "\n",
    "    if len(token_indices[1]) > 0:\n",
    "        last_occurrence_idx = token_indices[1][-1].item()\n",
    "        cropped_tensor = generated_ids[:, last_occurrence_idx + 1:]\n",
    "    else:\n",
    "        cropped_tensor = generated_ids\n",
    "\n",
    "    # Remove EOS tokens\n",
    "    processed_rows = []\n",
    "    for row in cropped_tensor:\n",
    "        # Apply the mask to each row\n",
    "        masked_row = row[row != token_to_remove]\n",
    "        processed_rows.append(masked_row)\n",
    "    \n",
    "    # Convert to code lists\n",
    "    code_lists = []\n",
    "    for row in processed_rows:\n",
    "        # Detect the end of meaningful audio content\n",
    "        # Orpheus typically uses values > 128266 for real audio content\n",
    "        # Look for sequences of low-value tokens that indicate silence/padding\n",
    "        row_list = row.tolist()\n",
    "        \n",
    "        # Find the end of meaningful content\n",
    "        meaningful_end_idx = len(row_list)\n",
    "        consecutive_low_values = 0\n",
    "        padding_threshold = 7  # Number of consecutive low-value tokens to consider as padding\n",
    "        \n",
    "        for i in range(len(row_list) - 1, -1, -1):\n",
    "            # Check if this looks like a padding or silence token\n",
    "            if row_list[i] < 128300:  # Adjust this threshold based on your model's token distribution\n",
    "                consecutive_low_values += 1\n",
    "            else:\n",
    "                consecutive_low_values = 0\n",
    "                \n",
    "            if consecutive_low_values >= padding_threshold:\n",
    "                meaningful_end_idx = i + padding_threshold\n",
    "                break\n",
    "        \n",
    "        # Trim to meaningful content and ensure it's a multiple of 7\n",
    "        trimmed_length = meaningful_end_idx - (meaningful_end_idx % 7)\n",
    "        if trimmed_length > 0:\n",
    "            trimmed_row = row_list[:trimmed_length]\n",
    "            # Convert to SNAC codes by subtracting the base offset\n",
    "            trimmed_row = [t - 128266 for t in trimmed_row]\n",
    "            code_lists.append(trimmed_row)\n",
    "        else:\n",
    "            # If we couldn't find meaningful content, use the original approach but limit length\n",
    "            row_length = row.size(0)\n",
    "            max_length = min(row_length, 210)  # Limit to ~3 seconds max (30 groups of 7)\n",
    "            new_length = (max_length // 7) * 7\n",
    "            trimmed_row = [t - 128266 for t in row[:new_length].tolist()]\n",
    "            code_lists.append(trimmed_row)\n",
    "\n",
    "    # Generate audio\n",
    "    my_samples = []\n",
    "    for code_list in code_lists:\n",
    "        samples = redistribute_codes(code_list, snac_model)\n",
    "        if samples is not None:\n",
    "            my_samples.append(samples)\n",
    "\n",
    "    return my_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_wav_from(samples: list) -> list[np.ndarray]:\n",
    "    \"\"\"Converts a list of PyTorch tensors (or NumPy arrays) to NumPy arrays.\"\"\"\n",
    "    processed_samples = []\n",
    "\n",
    "    for s in samples:\n",
    "        if isinstance(s, torch.Tensor):  # Check if it's a tensor\n",
    "            s = detect_and_remove_buzzing(s)\n",
    "            s = s.detach().squeeze().to('cpu').numpy()\n",
    "        else:  # Assume it's already a NumPy array\n",
    "            s = np.squeeze(s)\n",
    "\n",
    "        processed_samples.append(s)\n",
    "\n",
    "    return processed_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_tts(fpath_audio_ref, audio_ref_transcript, texts: list[str], model, snac_model, tokenizer):\n",
    "    inp_ids, attn_mask = prepare_inputs(fpath_audio_ref, audio_ref_transcript, texts, snac_model, tokenizer)\n",
    "    gen_ids = inference(model, inp_ids, attn_mask)\n",
    "    samples = convert_tokens_to_speech(gen_ids, snac_model)\n",
    "    wav_forms = to_wav_from(samples)\n",
    "    return wav_forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_wav(samples: list[np.array], sample_rate: int, filenames: list[str]):\n",
    "    \"\"\" Saves a list of tensors as .wav files.\n",
    "\n",
    "    Args:\n",
    "        samples (list[torch.Tensor]): List of audio tensors.\n",
    "        sample_rate (int): Sample rate in Hz.\n",
    "        filenames (list[str]): List of filenames to save.\n",
    "    \"\"\"\n",
    "    wav_data = to_wav_from(samples)\n",
    "\n",
    "    for data, filename in zip(wav_data, filenames):\n",
    "        write(filename, sample_rate, data.astype(np.float32))\n",
    "        print(f\"saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero shot: pre_audio/01.wav This British etiquette is often passed down through many generations from family to family to family, which has led to a very strong cultural emphasis on politeness. Politeness even in everyday interactions and being mindful about not causing inconvenience to your fellow human beings.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12848\\4139515134.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfpath_audio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maudio_transcript\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprompt_pairs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33mzero shot: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mfpath_audio\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maudio_transcript\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mwav_forms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzero_shot_tts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfpath_audio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maudio_transcript\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msnac_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m# import os\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12848\\2630066557.py\u001b[0m in \u001b[0;36mzero_shot_tts\u001b[1;34m(fpath_audio_ref, audio_ref_transcript, texts, model, snac_model, tokenizer)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0minp_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattn_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepare_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfpath_audio_ref\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maudio_ref_transcript\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msnac_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mgen_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minp_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_tokens_to_speech\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msnac_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mwav_forms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_wav_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwav_forms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12848\\1688416107.py\u001b[0m in \u001b[0;36mconvert_tokens_to_speech\u001b[1;34m(generated_ids, snac_model)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[0mmy_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcode_list\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcode_lists\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mredistribute_codes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msnac_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0mmy_samples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12848\\3987226406.py\u001b[0m in \u001b[0;36mredistribute_codes\u001b[1;34m(code_list, snac_model)\u001b[0m\n\u001b[0;32m     14\u001b[0m                         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0maudio_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msnac_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0maudio_hat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\orpheus\\.venv\\Lib\\site-packages\\snac\\snac.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, codes)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcodes\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         \u001b[0mz_q\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquantizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_codes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m         \u001b[0maudio_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_q\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0maudio_hat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\orpheus\\.venv\\Lib\\site-packages\\snac\\vq.py\u001b[0m in \u001b[0;36mfrom_codes\u001b[1;34m(self, codes)\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[0mz_q\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_codebooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m             \u001b[0mz_p_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquantizers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode_code\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m             \u001b[0mz_q_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquantizers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout_proj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_p_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[0mz_q_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mz_q_i\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat_interleave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquantizers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\orpheus\\.venv\\Lib\\site-packages\\snac\\vq.py\u001b[0m in \u001b[0;36mdecode_code\u001b[1;34m(self, embed_id)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode_code\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membed_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed_code\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membed_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode_latents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\orpheus\\.venv\\Lib\\site-packages\\snac\\vq.py\u001b[0m in \u001b[0;36membed_code\u001b[1;34m(self, embed_id)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0membed_code\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membed_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membed_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcodebook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode_code\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membed_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\orpheus\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2549\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2550\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2551\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"Throw this away.\",\n",
    "    \"He questioned what'll be the result of the experiment.\",\n",
    "    \"He wouldn't have done it if he knew the consequences.\",\n",
    "    \"If we weren't focused, we'd miss the opportunity.\",\n",
    "    \"She wouldn't even try to resolve the issue.\",\n",
    "    \"They weren't always this careful with their work.\",\n",
    "    # \"They were'nt participating in the survey.\",\n",
    "    # \"They've attended the conference.\",\n",
    "    # \"They've made their decision.\",\n",
    "    # \"We were'nt able to find the solution.\",\n",
    "    # \"We'd like to go hiking this weekend.\",\n",
    "    # \"We'd worked all day on the project.\",\n",
    "    # \"We're concerned about the upcoming deadline.\",\n",
    "]\n",
    "\n",
    "prompt_pairs = [\n",
    "    (\"pre_audio/01.wav\", \"This British etiquette is often passed down through many generations from family to family to family, which has led to a very strong cultural emphasis on politeness. Politeness even in everyday interactions and being mindful about not causing inconvenience to your fellow human beings.\"),\n",
    "    # (\"pre_audio/02.wav\", \"This is another phrase that will come in very handy for you all when it comes to sounding polite.\"),\n",
    "    # (\"pre_audio/03.wav\", \"A classic example is when you're at the dinner table and you ask someone to pass you the salt.\")\n",
    "]\n",
    "\n",
    "for fpath_audio, audio_transcript in prompt_pairs:\n",
    "    print(f\"zero shot: {fpath_audio} {audio_transcript}\")\n",
    "    wav_forms = zero_shot_tts(fpath_audio, audio_transcript, texts, model, snac_model, tokenizer)\n",
    "\n",
    "    # import os\n",
    "    from pathlib import Path\n",
    "\n",
    "\n",
    "    out_dir = Path('out')\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)  # Correct method\n",
    "    file_names = [f\"{out_dir.as_posix()}/{Path(fpath_audio).stem}_{i}.wav\" for i, t in enumerate(texts)]\n",
    "    save_wav(wav_forms, 24000, file_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
